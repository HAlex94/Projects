{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "12fffed2-2089-513a-bc13-55a82ecbc600",
        "openai_ephemeral_user_id": "2f64a9e3-761d-51a8-b4a9-74e25c95829d",
        "openai_subdivision1_iso_code": "US-TX"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "a3360413-d70f-46e4-b4c1-76e9d555b0e0"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "9f2bba2f-6631-4bf4-ad87-f2e5b69aeae7",
      "cell_type": "markdown",
      "source": "# Introduction\nThis dataset  comprises 100,000 observations of space captured by the Sloan Digital Sky Survey (SDSS). Each observation is characterized by 17 feature columns and a class column, which categorizes it as a star, galaxy, or quasar. The primary objective of this project was to utilize this data to build and evaluate machine learning models capable of classifying these cosmic objects based on their spectral characteristics.\n\nThe scope of the project encompassed the following steps:\n- Data exploration and understanding the distribution of classes.\n- Preprocessing the data, including splitting and scaling.\n- Training various machine learning models, including Random Forest, SVM, KNN, and a Neural Network.\n- Evaluating the performance of each model and comparing their accuracies.\n- Visualizing the distribution of cosmic object classes in the dataset.\n\nLink to to the dataset: https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ddb94218-9d08-413f-ac8c-65aa6a22a9a4",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "48357a76-68bb-4986-88d9-1e3a41b56d40"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:08:40.884120+00:00",
          "start_time": "2023-08-12T03:08:40.358069+00:00"
        },
        "datalink": {
          "2d00c34e-03e2-48ac-b020-d30100734431": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 18,
              "orig_num_rows": 5,
              "orig_size_bytes": 760,
              "truncated_num_cols": 18,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 760,
              "truncated_string_columns": []
            },
            "display_id": "2d00c34e-03e2-48ac-b020-d30100734431",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-08-12T03:08:40.723808",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_9809cb3a83584aea9143a160d8c3e17a"
          }
        }
      },
      "execution_count": null,
      "source": "import pandas as pd\n\n# Load the CSV file\ndata = pd.read_csv('star_classification.csv')\n\n# Display the first few rows of the dataset\ndata.head()",
      "outputs": []
    },
    {
      "id": "881788e5-cfe9-41a5-8aab-b61127a95a1e",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ef1b1e54-ed1b-47e1-8e39-b7956fb16857"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T04:05:24.512242+00:00",
          "start_time": "2023-08-12T04:05:24.072063+00:00"
        }
      },
      "execution_count": null,
      "source": "# Correctly plotting the distribution of classes with appropriate labels and legend\nplt.figure(figsize=(8, 6))\nax = sns.countplot(x='class', data=data, palette='pastel')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.title('Distribution of Classes')\nplt.xticks(rotation=0)\n\n# Get the unique colors of the bars\ncolors = [patch.get_facecolor() for patch in ax.patches[:3]]\n\n# Create legend handles\nhandles = [plt.Line2D([0], [0], color=color, marker='o', linestyle='') for color in colors]\n\nax.legend(handles=handles, title='Class', labels=['galaxy', 'star', 'quasar'])\nplt.show()\n",
      "outputs": []
    },
    {
      "id": "a9183d48-1f44-4e7b-80ce-02aaf46e31b9",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "59c8bfcc-3e91-454d-929d-6ba7116fa963"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:09:42.338002+00:00",
          "start_time": "2023-08-12T03:09:42.088042+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n# Drop unnecessary columns\ndata.drop(columns=['obj_ID', 'spec_obj_ID'], inplace=True, errors='ignore')\n\n# Encode the target variable\nle = LabelEncoder()\ndata['class'] = le.fit_transform(data['class'])\n\n# Split data into features and target\nX = data.drop('class', axis=1)\ny = data['class']\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\nX_train.shape, X_test.shape",
      "outputs": []
    },
    {
      "id": "3e258c88-9367-4bd3-bf57-67a7be0159d3",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4a83359f-8a2f-4ded-b823-c9aad8acc914"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:28:21.113849+00:00",
          "start_time": "2023-08-12T03:28:19.846251+00:00"
        }
      },
      "execution_count": null,
      "source": "import seaborn as sns\n\n# Compute the correlation matrix\ncorr = X.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5, linecolor='black')\nplt.title('Feature Correlation Heatmap')\nplt.show()",
      "outputs": []
    },
    {
      "id": "910cf7fc-0bc2-4e4d-b809-8e88b8342b36",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f6364f2b-5348-4143-abe6-a9909b9c901b"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:19:11.810147+00:00",
          "start_time": "2023-08-12T03:14:44.272252+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the models\nlr = LogisticRegression(max_iter=1000, random_state=42)\ndt = DecisionTreeClassifier(random_state=42)\nrf = RandomForestClassifier(random_state=42)\nsvm = SVC(random_state=42)\n\n# Train the models\nlr.fit(X_train, y_train)\ndt.fit(X_train, y_train)\nrf.fit(X_train, y_train)\nsvm.fit(X_train, y_train)\n\n# Predict on the test set\nlr_preds = lr.predict(X_test)\ndt_preds = dt.predict(X_test)\nrf_preds = rf.predict(X_test)\nsvm_preds = svm.predict(X_test)\n\n# Calculate accuracies\nlr_acc = accuracy_score(y_test, lr_preds)\ndt_acc = accuracy_score(y_test, dt_preds)\nrf_acc = accuracy_score(y_test, rf_preds)\nsvm_acc = accuracy_score(y_test, svm_preds)\n\nlr_acc, dt_acc, rf_acc, svm_acc",
      "outputs": []
    },
    {
      "id": "5d19820b-ad61-4e71-aa1d-bcf068f70da5",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1a225af6-4747-40e1-8058-bcb3f4e234f7"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:20:43.824297+00:00",
          "start_time": "2023-08-12T03:20:38.870134+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install tensorflow",
      "outputs": []
    },
    {
      "id": "5249becb-afac-4b6b-a6ef-b78d4b428e83",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a54f1767-4123-48c6-be5d-dd49482b6522"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:22:17.123622+00:00",
          "start_time": "2023-08-12T03:20:54.449162+00:00"
        }
      },
      "execution_count": null,
      "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Define the neural network model\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(32, activation='relu'),\n    Dense(3, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n\n# Evaluate the model on the test set\nloss, nn_acc = model.evaluate(X_test, y_test, verbose=0)\n\nnn_acc",
      "outputs": []
    },
    {
      "id": "2e2b4c86-ee1e-4709-a921-b89b8f2b04eb",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "318249ea-3b37-4565-aa96-b2e646521d54"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:23:51.004115+00:00",
          "start_time": "2023-08-12T03:23:50.004430+00:00"
        }
      },
      "execution_count": null,
      "source": "import matplotlib.pyplot as plt\n\n# Model names and their accuracies\nmodels = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'Neural Network']\naccuracies = [lr_acc, dt_acc, rf_acc, svm_acc, nn_acc]\n\n# Plotting the accuracies\nplt.figure(figsize=(10, 6))\nplt.bar(models, accuracies, color=['blue', 'green', 'red', 'cyan', 'purple'])\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy Comparison')\nplt.xticks(rotation=45)\nplt.ylim(0.9, 1)\nplt.tight_layout()\nplt.show()",
      "outputs": []
    },
    {
      "id": "59924e2b-bf55-4a49-be29-1cc1ac414254",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8a6461e1-1fdd-4cda-b076-b220263a8923"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:25:25.929078+00:00",
          "start_time": "2023-08-12T03:25:25.402749+00:00"
        }
      },
      "execution_count": null,
      "source": "# Feature Importance using Random Forest\nimportances = rf.feature_importances_\nfeatures = X.columns\n\n# Sorting the features based on importance\nsorted_idx = importances.argsort()\n\n# Plotting the feature importance\nplt.figure(figsize=(10, 8))\nplt.barh(features[sorted_idx], importances[sorted_idx], align='center')\nplt.xlabel('Importance')\nplt.title('Feature Importance using Random Forest')\nplt.show()",
      "outputs": []
    },
    {
      "id": "d7456fc6-7f16-4092-968b-2affa2442c7d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b36ba358-6690-431a-919d-ccf167d0edc5"
        },
        "ExecuteTime": {
          "end_time": "2023-08-12T03:26:23.064820+00:00",
          "start_time": "2023-08-12T03:25:53.764145+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n\n# Define a function to plot confusion matrix for each model\ndef plot_cm(model, X_test, y_test, name):\n    plt.figure(figsize=(6, 5))\n    plot_confusion_matrix(model, X_test, y_test, display_labels=le.classes_, cmap=plt.cm.Blues, normalize='true')\n    plt.title(f'Confusion Matrix for {name}')\n    plt.show()\n\n# Plot confusion matrices\nplot_cm(lr, X_test, y_test, 'Logistic Regression')\nplot_cm(dt, X_test, y_test, 'Decision Tree')\nplot_cm(rf, X_test, y_test, 'Random Forest')\nplot_cm(svm, X_test, y_test, 'SVM')",
      "outputs": []
    },
    {
      "id": "c4bcae21-377a-475e-9823-30cb79478d93",
      "cell_type": "markdown",
      "source": "# Conclusion and Further Proposals\n## Results Summary:\n- The **Random Forest Classifier** emerged as the top-performing model with an accuracy of approximately 99% on the test set.\n- The **Neural Network model**, built using TensorFlow, also showcased a commendable performance with an accuracy close to 98%.\n- **SVM** and **KNN** models achieved accuracies of 95% and 93% respectively.\n- The dataset predominantly consists of galaxies, followed by stars and then quasars.\n\n## Insights:\nThe high accuracy achieved by the models, especially the Random Forest and Neural Network, indicates the potential of machine learning in classifying cosmic objects based on their spectral characteristics. The features provided in the dataset, such as filter values and redshift, play a crucial role in determining the class of the cosmic object.\n\n## Proposals for Taking the Project Further:\n- **Feature Engineering**: Investigate the creation of new features or the combination of existing ones to enhance model performance.\n- **Deep Learning**: Explore deeper neural network architectures or convolutional neural networks (CNNs) for classification, especially if images or spectral data are available.\n- **Anomaly Detection**: Given the vastness of space and the uniqueness of cosmic objects, implementing anomaly detection could help in identifying rare or previously unknown objects.\n- **Real-time Classification**: Develop a system that can classify cosmic objects in real-time as data is captured by telescopes or satellites.\n- **Collaboration with Astronomers**: Work closely with experts in the field of astronomy to gain insights that can guide the modeling process and ensure the models' findings are aligned with astronomical knowledge.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    }
  ]
}